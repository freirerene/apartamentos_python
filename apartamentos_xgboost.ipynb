{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost for predicting rent price\n",
    "\n",
    "In our [previous model](https://github.com/freirerene/apartamentos_python/blob/main/random_forest.ipynb) we applied a random forest regressor to the data we collect on the rent price of apartaments in São Paulo.\n",
    "\n",
    "In this notebook we're going to apply [xgboost](https://xgboost.readthedocs.io/en/latest/), which is an open source implementation of gradient boosting. Gradient boosting is an improvement upon random forest: while random forests build a bunch of decision trees from randomly selected subsets of features and samples to build a tree, and in the end avarage them out, gradient boost builds each tree at a time, and each tree improves upon an earlier tree.\n",
    "\n",
    "We chose to build this model using AWS because its faster this way -- we can do hyperparameter tuning in a couple of hours, as opposed to whole weeks.\n",
    "\n",
    "Now, working on AWS is a bit different than simply running a jupyter notebook locally. First of all, we have to initiate the session within sagemaker, create a IAM (identify and access management) role and create an s3 (simple storage service) storage -- s3 is where sagemaker stores trainig/testing variables, as well as trainig jobs, hyperparameter tuning jobs, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "# Define IAM role\n",
    "import boto3\n",
    "import re\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "prefix = 'sagemaker/apartaments-xgboost'\n",
    "region = boto3.Session().region_name\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "sm_client = boto3.Session().client('sagemaker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to select the best parameters, its easier to simply apply RFECV on the standard python implementation of xgboost. So we install the xgboost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.3.3-py3-none-manylinux2010_x86_64.whl (157.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 157.5 MB 26 kB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from xgboost) (1.19.5)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from xgboost) (1.4.1)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.3.3\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we go on to importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from IPython.display import display\n",
    "from time import strftime, gmtime\n",
    "import pickle\n",
    "\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.tuner import IntegerParameter\n",
    "from sagemaker.tuner import ContinuousParameter\n",
    "from sagemaker.tuner import HyperparameterTuner\n",
    "\n",
    "import xgboost\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.feature_selection import RFECV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the data and exclude the outliers, as well as restrict the area to 200 squared meters -- as we did in our other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://github.com/freirerene/apartamentos_python/raw/main/apartamentos_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = ['aluguel', 'condominio', 'area']\n",
    "df = df[(np.abs(stats.zscore(df[con])) < 3).all(axis=1)]\n",
    "df_b = pd.get_dummies(df, columns=['bairro'])\n",
    "smaller_b = df_b[df_b['area'] <= 200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define two functions we'll use below: a `print_mae`, whose only purpose is to print the MAE of the test and training set and the difference between them; the `predict` function, which separates the dataset we want to predict into chunks and applies our predictor to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, model, rows=500):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in split_array:\n",
    "        predictions = ','.join([predictions, model.predict(array).decode('utf-8')])\n",
    "\n",
    "    return np.fromstring(predictions[1:], sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_mae(y_test, y_train, y_p_test, y_p_train):\n",
    "    mae_train = mean_absolute_error(y_train, y_p_train)\n",
    "    mae_test = mean_absolute_error(y_test, y_p_test)\n",
    "    spread = np.abs(mae_train - mae_test)\n",
    "    print(\"MAE train: {:0.2f}\".format(mae_train))\n",
    "    print(\"-\"*100)\n",
    "    print(\"MAE test: {:0.2f}\".format(mae_test))\n",
    "    print(\"-\"*100)\n",
    "    print(\"Spread: {:0.2f}\".format(spread))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select the features we'll use we will simply run an `XGBRegressor` with default parameters on a RFECV. What RFECV does is quite simple: it tests a combination of features and recursively excludes the combinations that don't produce good results. And it does this using cross validation.\n",
    "\n",
    "First we define our features `x` and our target variable `y` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = smaller_b.drop(['aluguel', 'anunciante', 'condominio'], axis=1)\n",
    "y = smaller_b['aluguel'] + smaller_b['condominio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "xgb_f = xgboost.XGBRegressor()\n",
    "\n",
    "selector = RFECV(xgb_f, scoring='neg_mean_absolute_error', cv=3)\n",
    "selector.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll save the results on a `.data` file, so we can use it later without having to run this `selector.fit` again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_columns = x.columns[selector.support_]\n",
    "\n",
    "with open('optimized_columns.data', 'wb') as filehandle:\n",
    "    pickle.dump(optimized_columns, filehandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the two cells below we load the data we saved earlier and restrict the features to the ones we selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('optimized_columns.data', 'rb') as filehandle:\n",
    "    optimized_columns = pickle.load(filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x[optimized_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train set and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our model we create two datasets: `train` and `test`, and we split the df using the usual sklearn's `train_test_split`. AWS's xgboost requires that the target variable is in the first column and that there's no label row, so we concat the `y`s and `X`s the way we did and we saved the data into `csv` without the header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([y_train, X_train], axis=1)\n",
    "test = pd.concat([y_test, X_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train.csv', header=False, index=False)\n",
    "test.to_csv('validation.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we load the data into an s3 instance and load it back as an s3 input variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv')).upload_file('validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = TrainingInput(s3_data='s3://{}/{}/train'.format(bucket, prefix), content_type='csv')\n",
    "s3_input_validation = TrainingInput(s3_data='s3://{}/{}/validation/'.format(bucket, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "\n",
    "Now we initiate xgboost sagemaker's built-in algorithm and create an estimator -- in the estimator we can use a slightly better instance (our base plan is `ml.t2.medium`, which is the most basic, but in some jobs AWS allows us to jump to better instances and then jump basic to our basic instance. This is good for price optimization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = sagemaker.image_uris.retrieve('xgboost', region, version='latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    instance_count=1, \n",
    "                                    instance_type='ml.m5.xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=sess)\n",
    "\n",
    "\n",
    "xgb.set_hyperparameters(objective='reg:linear', num_round=100)\n",
    "\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we deploy our model (this crates an endpoint that can be used in other notebooks, and so on) and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "xgb_predictor = xgb.deploy(\n",
    "    initial_instance_count = 1, \n",
    "    instance_type = 'ml.t2.medium',\n",
    "    serializer=CSVSerializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE train: 622.33\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MAE test: 724.16\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Spread: 101.83\n"
     ]
    }
   ],
   "source": [
    "predictions_train = predict(train.to_numpy()[:,1:], xgb_predictor)\n",
    "predictions_test = predict(test.to_numpy()[:,1:], xgb_predictor)\n",
    "print_mae(y_test, y_train, predictions_test, predictions_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that these results are already better than the best results using random forests. Now let's see if we can improve it doing some hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "To perform the tuning we use AWS' `HyperparameterTuner`. We simply define the hyperparameter's range in a dictionary. With `tuner_log` we define the job then fit it to our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the best result in our sagemaker's dashboard, in the `Training -> Hyperparameter tuning jobs` section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {\n",
    "    'alpha': ContinuousParameter(0, 1000, scaling_type=\"Auto\"),\n",
    "    'subsample': ContinuousParameter(0.5,1,scaling_type='Logarithmic'),\n",
    "    'num_round': IntegerParameter(1, 4000, scaling_type=\"Auto\"),\n",
    "    'min_child_weight': ContinuousParameter(0, 120, scaling_type=\"Auto\")\n",
    "}\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    xgb,\n",
    "    objective_metric_name = 'validation:mae',\n",
    "    objective_type ='Minimize',\n",
    "    hyperparameter_ranges = hyperparameter_ranges,\n",
    "    max_jobs=150,\n",
    "    max_parallel_jobs=4,\n",
    "    strategy='Bayesian'\n",
    ")\n",
    "\n",
    "tuner.fit({'train': s3_input_train, 'validation': s3_input_validation}, include_cls_metadata=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the results from the tuning we do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_job_result = sm_client.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName = 'xgboost-210227-0429')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can also get the name of the (last) tuning job with the command `tuner.latest_tuning_job.job_name`.\n",
    "\n",
    "And, finally, to see the best parameters from the tuning job we simply have to find the right keys in the dictionary `tuning_job_result`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': '0.0',\n",
       " 'min_child_weight': '0.14731765645826228',\n",
       " 'num_round': '807',\n",
       " 'subsample': '0.949738441523912'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuning_job_result['BestTrainingJob']['TunedHyperParameters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    instance_count=1, \n",
    "                                    instance_type='ml.m5.xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=sess)\n",
    "\n",
    "\n",
    "xgb.set_hyperparameters(objective='reg:linear',\n",
    "                        alpha=0.0,\n",
    "                        min_child_weight=0.14731765645826228,\n",
    "                        subsample=0.949738441523912,\n",
    "                        num_round=807)\n",
    "\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------!"
     ]
    }
   ],
   "source": [
    "xgb_predictor_opt = xgb.deploy(\n",
    "    initial_instance_count = 1, \n",
    "    instance_type = 'ml.t2.medium',\n",
    "    serializer=CSVSerializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE train: 355.60\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MAE test: 648.89\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Spread: 293.29\n"
     ]
    }
   ],
   "source": [
    "predictions_train = predict(train.to_numpy()[:,1:], xgb_predictor_opt)\n",
    "predictions_test = predict(test.to_numpy()[:,1:], xgb_predictor_opt)\n",
    "print_mae(y_test, y_train, predictions_test, predictions_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor_opt.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two straightforward ways to control overfitting adjusting the hyperparameters: first, controlling the model complexity by adjusting the `max_depth` and the `gamma` (aka `min_split_loss`); second, by making the model more robust to noises, by adjusting `colsample_bytree`.\n",
    "\n",
    "There are other parameters that control model complexity and its behaviour with noises, and some of them were already optimized in the tuning job earlier. This actually gives a way better result compared to just tuning every single parameter possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some experimentation we saw that a good compromise in the bias-variance tradeoff is to set `max_depth=5` and `gamma=5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb2 = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    instance_count=1, \n",
    "                                    instance_type='ml.m5.xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=sess)\n",
    "\n",
    "\n",
    "xgb2.set_hyperparameters(objective='reg:linear',\n",
    "                        gamma=5,\n",
    "                        max_depth=5,\n",
    "                        alpha=0.0,\n",
    "                        min_child_weight=0.14731765645826228,\n",
    "                        subsample=0.949738441523912,\n",
    "                        num_round=807)\n",
    "\n",
    "xgb2.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "xgb_predictor2 = xgb2.deploy(\n",
    "    initial_instance_count = 1, \n",
    "    instance_type = 'ml.t2.medium',\n",
    "    serializer=CSVSerializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE train: 413.48\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MAE test: 662.50\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Spread: 249.02\n"
     ]
    }
   ],
   "source": [
    "predictions_train = predict(train.to_numpy()[:,1:], xgb_predictor2)\n",
    "predictions_test = predict(test.to_numpy()[:,1:], xgb_predictor2)\n",
    "print_mae(y_test, y_train, predictions_test, predictions_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor2.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in the complexity control, we saw from experimentation that the best `colsample_bytree` is `1.0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb3 = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    instance_count=1, \n",
    "                                    instance_type='ml.m5.xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=sess)\n",
    "\n",
    "\n",
    "xgb3.set_hyperparameters(objective='reg:linear',\n",
    "                        gamma=5,\n",
    "                        max_depth=5,\n",
    "                        alpha=0.0,\n",
    "                        min_child_weight=0.14731765645826228,\n",
    "                        subsample=0.949738441523912,\n",
    "                        colsample_bytree=1.0,\n",
    "                        num_round=807)\n",
    "\n",
    "xgb3.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------!"
     ]
    }
   ],
   "source": [
    "xgb_predictor3 = xgb3.deploy(\n",
    "    initial_instance_count = 1, \n",
    "    instance_type = 'ml.t2.medium',\n",
    "    serializer=CSVSerializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE train: 413.48\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MAE test: 662.50\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Spread: 249.02\n"
     ]
    }
   ],
   "source": [
    "predictions_train = predict(train.to_numpy()[:,1:], xgb_predictor3)\n",
    "predictions_test = predict(test.to_numpy()[:,1:], xgb_predictor3)\n",
    "print_mae(y_test, y_train, predictions_test, predictions_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we poke around the model we could improve it further, but the results are quite good -- we were able to decrease the MAE by `20%`, comparing our best random forest model with our best xgboost model, even though the spread is a tad higher than ideal.\n",
    "\n",
    "We end the project with two endpoints: `xgb_predictor` and `xgb_predictor3`.  The first represents the model without any optimized parameter; the second represents the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
